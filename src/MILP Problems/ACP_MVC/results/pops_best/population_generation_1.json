{
     "algorithm": "An adaptive randomized neighborhood selection algorithm that scores variables based on their objective contribution, constraint slackness, correlation via shared constraints, and a random perturbation to balance exploration and exploitation.",
     "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    # 1. Objective-based score: higher absolute coefficient -> more impact\n    obj_score = np.abs(objective_coefficient)\n    obj_score = obj_score / (obj_score.max() + 1e-10)\n    \n    # 2. Slackness score: variables in tight constraints are more critical\n    slackness = np.zeros(n)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        slack = constraint[i] - lhs\n        if slack < 1e-6:  # tight constraint\n            for j in range(k[i]):\n                slackness[site[i][j]] += 1\n    if slackness.max() > 0:\n        slackness = slackness / slackness.max()\n    \n    # 3. Correlation score: variables appearing together in many constraints\n    correlation = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            correlation[site[i][j]] += k[i]  # more variables in constraint -> higher correlation potential\n    if correlation.max() > 0:\n        correlation = correlation / correlation.max()\n    \n    # 4. Change score: variables that differ from initial solution\n    change = np.abs(current_solution - initial_solution)\n    \n    # 5. Random perturbation to avoid local optima\n    random_perturb = np.random.rand(n)\n    \n    # Combine scores with weights\n    neighbor_score = (0.3 * obj_score + \n                      0.3 * slackness + \n                      0.2 * correlation + \n                      0.1 * change + \n                      0.1 * random_perturb)\n    \n    return neighbor_score",
     "objective": 6053.61845,
     "other_inf": null
}