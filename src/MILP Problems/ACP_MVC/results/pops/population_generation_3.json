[
     {
          "algorithm": "An ensemble scoring algorithm that combines multiple independent scoring strategies (including constraint violation potential, variable flipping impact, and random walk) with a dynamic weighting mechanism based on recent improvement trends.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    strategy_scores = []\n    strategy_weights = np.ones(3)\n    \n    # Strategy 1: Constraint violation potential\n    violation_potential = np.zeros(n)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        slack = constraint[i] - lhs\n        if slack < 0:\n            for j in range(k[i]):\n                idx = site[i][j]\n                violation_potential[idx] += abs(value[i][j]) * abs(slack)\n    if violation_potential.max() > 0:\n        violation_potential = violation_potential / violation_potential.max()\n    strategy_scores.append(violation_potential)\n    \n    # Strategy 2: Variable flipping impact (estimated objective change)\n    flip_impact = np.zeros(n)\n    for idx in range(n):\n        if current_solution[idx] == 0:\n            flip_impact[idx] = objective_coefficient[idx]\n        else:\n            flip_impact[idx] = -objective_coefficient[idx]\n    flip_impact = np.abs(flip_impact)\n    if flip_impact.max() > 0:\n        flip_impact = flip_impact / flip_impact.max()\n    strategy_scores.append(flip_impact)\n    \n    # Strategy 3: Random walk with memory of recent changes\n    recent_change = np.abs(current_solution - initial_solution)\n    random_component = np.random.rand(n)\n    memory_random = 0.7 * random_component + 0.3 * recent_change\n    strategy_scores.append(memory_random)\n    \n    # Dynamic weighting based on correlation between strategies\n    correlation_matrix = np.zeros((3, 3))\n    for i in range(3):\n        for j in range(3):\n            if i != j:\n                corr = np.corrcoef(strategy_scores[i], strategy_scores[j])[0,1]\n                correlation_matrix[i,j] = 0 if np.isnan(corr) else abs(corr)\n    \n    diversity_score = 1 - np.mean(correlation_matrix, axis=1)\n    strategy_weights = diversity_score / diversity_score.sum()\n    \n    # Combine strategies with dynamic weights\n    for i in range(3):\n        neighbor_score += strategy_weights[i] * strategy_scores[i]\n    \n    # Add small noise for exploration\n    neighbor_score += 0.05 * np.random.randn(n)\n    \n    return neighbor_score",
          "objective": 5593.0294,
          "other_inf": null
     },
     {
          "algorithm": "Graph-based variable scoring using constraint-induced variable correlations, reduced cost impact, and a diversification mechanism based on constraint activity and random perturbation.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    # Build constraint-variable adjacency matrix\n    constraint_activity = np.zeros(m)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        slack = constraint[i] - lhs\n        constraint_activity[i] = max(0, 1.0 - slack)  # 1 if tight, 0 if loose\n    \n    # 1. Constraint-induced correlation score\n    correlation_score = np.zeros(n)\n    for i in range(m):\n        if constraint_activity[i] > 0.5:  # Focus on active constraints\n            for j in range(k[i]):\n                var_idx = site[i][j]\n                correlation_score[var_idx] += constraint_activity[i] * np.abs(value[i][j])\n    \n    # 2. Reduced cost inspired score (for binary variables)\n    reduced_cost_score = np.zeros(n)\n    for var in range(n):\n        if current_solution[var] == 0:\n            reduced_cost_score[var] = max(0, -objective_coefficient[var])\n        else:\n            reduced_cost_score[var] = max(0, objective_coefficient[var])\n    \n    # 3. Constraint participation diversity\n    participation_count = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            participation_count[site[i][j]] += 1\n    \n    # 4. Random perturbation with memory of previous changes\n    change_history = np.abs(current_solution - initial_solution)\n    perturbation = np.random.rand(n) * (1.0 + change_history)  # More random for changed variables\n    \n    # Normalize components\n    if correlation_score.max() > 0:\n        correlation_score = correlation_score / correlation_score.max()\n    if reduced_cost_score.max() > 0:\n        reduced_cost_score = reduced_cost_score / reduced_cost_score.max()\n    if participation_count.max() > 0:\n        participation_count = participation_count / participation_count.max()\n    perturbation = perturbation / perturbation.max() if perturbation.max() > 0 else perturbation\n    \n    # Combine scores with adaptive weights\n    active_constraint_ratio = np.sum(constraint_activity > 0.5) / max(m, 1)\n    weight_correlation = 0.4 + 0.2 * active_constraint_ratio\n    weight_reduced = 0.3 - 0.1 * active_constraint_ratio\n    \n    neighbor_score = (weight_correlation * correlation_score +\n                      weight_reduced * reduced_cost_score +\n                      0.1 * participation_count +\n                      0.2 * perturbation)\n    \n    return neighbor_score",
          "objective": 5765.04694,
          "other_inf": null
     },
     {
          "algorithm": "An adaptive neighborhood selection algorithm that scores variables based on a weighted combination of objective sensitivity, constraint slackness, historical solution changes, and a random perturbation for exploration, then uses a probabilistic selection via softmax with an adaptive temperature.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    constraint_slack = np.zeros(m)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        constraint_slack[i] = max(0, constraint[i] - lhs)\n    var_slack_contribution = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            var_slack_contribution[var_idx] += constraint_slack[i] * abs(value[i][j])\n    obj_sensitivity = np.abs(objective_coefficient)\n    change_history = np.abs(current_solution - initial_solution)\n    random_perturbation = np.random.rand(n)\n    base_score = (0.35 * obj_sensitivity + \n                  0.25 * var_slack_contribution + \n                  0.2 * change_history + \n                  0.2 * random_perturbation)\n    avg_score = np.mean(base_score)\n    temperature = max(0.1, 0.5 * avg_score)\n    exp_scores = np.exp(base_score / temperature)\n    neighbor_score = exp_scores / np.sum(exp_scores)\n    return neighbor_score",
          "objective": 5809.86535,
          "other_inf": null
     },
     {
          "algorithm": "An adaptive neighborhood selection algorithm that scores variables based on objective contribution, constraint violation potential, historical frequency of change, and a simulated annealing-inspired random component.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    # 1. Objective contribution (absolute coefficient)\n    obj_score = np.abs(objective_coefficient)\n    if obj_score.max() > 0:\n        obj_score = obj_score / obj_score.max()\n    \n    # 2. Constraint violation potential: variables in constraints with large slack are less critical\n    violation_potential = np.zeros(n)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        slack = constraint[i] - lhs\n        # Negative slack means violation, but for feasible solution slack >= 0.\n        # We focus on constraints with small slack (almost tight).\n        tightness = max(0, 1.0 - slack)  # 1 if slack=0, decreases as slack increases\n        for j in range(k[i]):\n            violation_potential[site[i][j]] += tightness * np.abs(value[i][j])\n    if violation_potential.max() > 0:\n        violation_potential = violation_potential / violation_potential.max()\n    \n    # 3. Historical change frequency (simulated by tracking differences from initial)\n    change_freq = np.abs(current_solution - initial_solution)\n    \n    # 4. Simulated annealing style random component (temperature decreases with iteration, but here we use fixed randomness with bias)\n    # We create a random bias that prefers variables not recently changed (low change_freq) for exploration\n    exploration_bias = np.random.rand(n) * (1.0 - change_freq)  # higher random weight for less changed variables\n    \n    # 5. Combine scores with adaptive weights\n    # Weight for violation potential increases if many constraints are tight\n    tight_count = sum(1 for i in range(m) if abs(constraint[i] - sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))) < 1e-6)\n    weight_violation = 0.2 + 0.2 * (tight_count / max(m, 1))  # between 0.2 and 0.4\n    \n    neighbor_score = (0.3 * obj_score +\n                      weight_violation * violation_potential +\n                      0.2 * change_freq +\n                      0.3 * exploration_bias)\n    \n    return neighbor_score",
          "objective": 5884.56186,
          "other_inf": null
     }
]