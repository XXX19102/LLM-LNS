[
     {
          "algorithm": "An adaptive randomized neighborhood selection algorithm that scores variables based on their objective contribution, constraint slackness, correlation via shared constraints, and a random perturbation to balance exploration and exploitation.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    # 1. Objective-based score: higher absolute coefficient -> more impact\n    obj_score = np.abs(objective_coefficient)\n    obj_score = obj_score / (obj_score.max() + 1e-10)\n    \n    # 2. Slackness score: variables in tight constraints are more critical\n    slackness = np.zeros(n)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        slack = constraint[i] - lhs\n        if slack < 1e-6:  # tight constraint\n            for j in range(k[i]):\n                slackness[site[i][j]] += 1\n    if slackness.max() > 0:\n        slackness = slackness / slackness.max()\n    \n    # 3. Correlation score: variables appearing together in many constraints\n    correlation = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            correlation[site[i][j]] += k[i]  # more variables in constraint -> higher correlation potential\n    if correlation.max() > 0:\n        correlation = correlation / correlation.max()\n    \n    # 4. Change score: variables that differ from initial solution\n    change = np.abs(current_solution - initial_solution)\n    \n    # 5. Random perturbation to avoid local optima\n    random_perturb = np.random.rand(n)\n    \n    # Combine scores with weights\n    neighbor_score = (0.3 * obj_score + \n                      0.3 * slackness + \n                      0.2 * correlation + \n                      0.1 * change + \n                      0.1 * random_perturb)\n    \n    return neighbor_score",
          "objective": 6053.61845,
          "other_inf": null
     },
     {
          "algorithm": "An adaptive randomized neighborhood selection algorithm that scores variables based on their objective contribution, constraint slackness, correlation via shared constraints, and a random perturbation to balance exploration and exploitation.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    # 1. Objective-based score: higher absolute coefficient gets higher score\n    obj_score = np.abs(objective_coefficient)\n    obj_score = obj_score / (np.max(obj_score) + 1e-10)\n    \n    # 2. Constraint slackness score: variables in tight constraints get higher score\n    slack_score = np.zeros(n)\n    for i in range(m):\n        lhs = 0.0\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            lhs += value[i][j] * current_solution[var_idx]\n        slack = constraint[i] - lhs\n        if slack < 1e-6:  # tight constraint\n            for j in range(k[i]):\n                var_idx = site[i][j]\n                slack_score[var_idx] += 1.0 / k[i]\n    slack_score = slack_score / (np.max(slack_score) + 1e-10)\n    \n    # 3. Correlation score: variables appearing together in constraints get similar scores\n    corr_score = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            corr_score[var_idx] += k[i]  # more variables in constraint -> higher correlation potential\n    \n    # 4. Change score: variables that changed from initial solution get higher score\n    change_score = np.abs(current_solution - initial_solution)\n    \n    # 5. Random perturbation to avoid local optima\n    random_score = np.random.rand(n)\n    \n    # Combine scores with weights\n    neighbor_score = (0.3 * obj_score + \n                      0.3 * slack_score + \n                      0.2 * corr_score / (np.max(corr_score) + 1e-10) + \n                      0.1 * change_score + \n                      0.1 * random_score)\n    \n    return neighbor_score",
          "objective": 6332.68764,
          "other_inf": null
     },
     {
          "algorithm": "This algorithm scores variables by combining their reduced costs, constraint slackness, correlation via shared constraints, and random perturbation to guide neighborhood selection for LNS.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    if n == 0:\n        return neighbor_score\n    \n    # 1. Reduced cost contribution (absolute value, higher means more potential improvement)\n    reduced_cost = np.abs(objective_coefficient)\n    \n    # 2. Constraint slackness: variables in tight constraints get higher scores\n    constraint_slack = np.zeros(m)\n    for i in range(m):\n        lhs = 0.0\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            lhs += value[i][j] * current_solution[var_idx]\n        constraint_slack[i] = constraint[i] - lhs\n    tightness_factor = np.zeros(n)\n    for i in range(m):\n        if constraint_slack[i] < 1e-6:  # tight constraint\n            for j in range(k[i]):\n                var_idx = site[i][j]\n                tightness_factor[var_idx] += 1.0\n    \n    # 3. Correlation via shared constraints: score increases with variables appearing together\n    correlation_factor = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            correlation_factor[var_idx] += k[i]  # more variables in constraint -> higher correlation\n    \n    # 4. Random perturbation to avoid local optima\n    random_factor = np.random.rand(n)\n    \n    # 5. Combine factors with weights\n    neighbor_score = (0.4 * reduced_cost / (np.max(reduced_cost) + 1e-9) +\n                      0.3 * tightness_factor / (np.max(tightness_factor) + 1e-9) +\n                      0.2 * correlation_factor / (np.max(correlation_factor) + 1e-9) +\n                      0.1 * random_factor)\n    \n    return neighbor_score",
          "objective": 6392.2376,
          "other_inf": null
     },
     {
          "algorithm": "An adaptive randomized neighborhood selection algorithm that scores variables based on their objective contribution, constraint violation potential, and correlation within constraints, then applies a softmax with temperature for probabilistic selection.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    constraint_activity = np.zeros(m)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        constraint_activity[i] = max(0, lhs - constraint[i])\n    var_constraint_violation = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            var_constraint_violation[var_idx] += constraint_activity[i] * abs(value[i][j])\n    obj_weight = np.abs(objective_coefficient)\n    change_from_initial = np.abs(current_solution - initial_solution)\n    correlation_score = np.zeros(n)\n    for i in range(m):\n        if k[i] > 1:\n            for j in range(k[i]):\n                var_idx = site[i][j]\n                correlation_score[var_idx] += k[i]\n    base_score = (0.4 * obj_weight + 0.3 * var_constraint_violation + \n                  0.2 * change_from_initial + 0.1 * correlation_score)\n    temperature = 0.5\n    exp_scores = np.exp(base_score / temperature)\n    neighbor_score = exp_scores / np.sum(exp_scores)\n    return neighbor_score",
          "objective": 7101.9453,
          "other_inf": null
     }
]