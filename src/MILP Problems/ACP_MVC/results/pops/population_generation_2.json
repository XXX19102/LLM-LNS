[
     {
          "algorithm": "An adaptive neighborhood selection algorithm that scores variables based on a weighted combination of objective sensitivity, constraint slackness, historical solution changes, and a random perturbation for exploration, then uses a probabilistic selection via softmax with an adaptive temperature.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    constraint_slack = np.zeros(m)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        constraint_slack[i] = max(0, constraint[i] - lhs)\n    var_slack_contribution = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            var_slack_contribution[var_idx] += constraint_slack[i] * abs(value[i][j])\n    obj_sensitivity = np.abs(objective_coefficient)\n    change_history = np.abs(current_solution - initial_solution)\n    random_perturbation = np.random.rand(n)\n    base_score = (0.35 * obj_sensitivity + \n                  0.25 * var_slack_contribution + \n                  0.2 * change_history + \n                  0.2 * random_perturbation)\n    avg_score = np.mean(base_score)\n    temperature = max(0.1, 0.5 * avg_score)\n    exp_scores = np.exp(base_score / temperature)\n    neighbor_score = exp_scores / np.sum(exp_scores)\n    return neighbor_score",
          "objective": 5809.86535,
          "other_inf": null
     },
     {
          "algorithm": "An adaptive neighborhood selection algorithm that scores variables based on objective contribution, constraint violation potential, historical frequency of change, and a simulated annealing-inspired random component.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    # 1. Objective contribution (absolute coefficient)\n    obj_score = np.abs(objective_coefficient)\n    if obj_score.max() > 0:\n        obj_score = obj_score / obj_score.max()\n    \n    # 2. Constraint violation potential: variables in constraints with large slack are less critical\n    violation_potential = np.zeros(n)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        slack = constraint[i] - lhs\n        # Negative slack means violation, but for feasible solution slack >= 0.\n        # We focus on constraints with small slack (almost tight).\n        tightness = max(0, 1.0 - slack)  # 1 if slack=0, decreases as slack increases\n        for j in range(k[i]):\n            violation_potential[site[i][j]] += tightness * np.abs(value[i][j])\n    if violation_potential.max() > 0:\n        violation_potential = violation_potential / violation_potential.max()\n    \n    # 3. Historical change frequency (simulated by tracking differences from initial)\n    change_freq = np.abs(current_solution - initial_solution)\n    \n    # 4. Simulated annealing style random component (temperature decreases with iteration, but here we use fixed randomness with bias)\n    # We create a random bias that prefers variables not recently changed (low change_freq) for exploration\n    exploration_bias = np.random.rand(n) * (1.0 - change_freq)  # higher random weight for less changed variables\n    \n    # 5. Combine scores with adaptive weights\n    # Weight for violation potential increases if many constraints are tight\n    tight_count = sum(1 for i in range(m) if abs(constraint[i] - sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))) < 1e-6)\n    weight_violation = 0.2 + 0.2 * (tight_count / max(m, 1))  # between 0.2 and 0.4\n    \n    neighbor_score = (0.3 * obj_score +\n                      weight_violation * violation_potential +\n                      0.2 * change_freq +\n                      0.3 * exploration_bias)\n    \n    return neighbor_score",
          "objective": 5884.56186,
          "other_inf": null
     },
     {
          "algorithm": "An adaptive randomized neighborhood selection algorithm that scores variables based on their objective contribution, constraint slackness, correlation via shared constraints, and a random perturbation to balance exploration and exploitation.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    \n    # 1. Objective-based score: higher absolute coefficient -> more impact\n    obj_score = np.abs(objective_coefficient)\n    obj_score = obj_score / (obj_score.max() + 1e-10)\n    \n    # 2. Slackness score: variables in tight constraints are more critical\n    slackness = np.zeros(n)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        slack = constraint[i] - lhs\n        if slack < 1e-6:  # tight constraint\n            for j in range(k[i]):\n                slackness[site[i][j]] += 1\n    if slackness.max() > 0:\n        slackness = slackness / slackness.max()\n    \n    # 3. Correlation score: variables appearing together in many constraints\n    correlation = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            correlation[site[i][j]] += k[i]  # more variables in constraint -> higher correlation potential\n    if correlation.max() > 0:\n        correlation = correlation / correlation.max()\n    \n    # 4. Change score: variables that differ from initial solution\n    change = np.abs(current_solution - initial_solution)\n    \n    # 5. Random perturbation to avoid local optima\n    random_perturb = np.random.rand(n)\n    \n    # Combine scores with weights\n    neighbor_score = (0.3 * obj_score + \n                      0.3 * slackness + \n                      0.2 * correlation + \n                      0.1 * change + \n                      0.1 * random_perturb)\n    \n    return neighbor_score",
          "objective": 6053.61845,
          "other_inf": null
     },
     {
          "algorithm": "An adaptive neighborhood selection algorithm that scores variables based on a weighted combination of objective sensitivity, constraint slackness, historical solution changes, and a randomized perturbation for exploration, then uses a probabilistic selection via softmax with an adaptive temperature.",
          "code": "import numpy as np\n\ndef select_neighborhood(n, m, k, site, value, constraint, initial_solution, current_solution, objective_coefficient):\n    neighbor_score = np.zeros(n)\n    constraint_slack = np.zeros(m)\n    for i in range(m):\n        lhs = sum(value[i][j] * current_solution[site[i][j]] for j in range(k[i]))\n        constraint_slack[i] = max(0, constraint[i] - lhs)\n    var_slack_contribution = np.zeros(n)\n    for i in range(m):\n        for j in range(k[i]):\n            var_idx = site[i][j]\n            var_slack_contribution[var_idx] += constraint_slack[i] * abs(value[i][j])\n    obj_sensitivity = np.abs(objective_coefficient)\n    change_magnitude = np.abs(current_solution - initial_solution)\n    perturbation = np.random.rand(n) * 0.1\n    base_score = (0.35 * obj_sensitivity + 0.25 * var_slack_contribution +\n                  0.25 * change_magnitude + 0.15 * perturbation)\n    avg_score = np.mean(base_score)\n    temperature = max(0.1, 0.5 * avg_score)\n    exp_scores = np.exp(base_score / temperature)\n    neighbor_score = exp_scores / np.sum(exp_scores)\n    return neighbor_score",
          "objective": 6089.102,
          "other_inf": null
     }
]